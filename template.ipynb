{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/annariha/StanCon-2024-BO-Stan/blob/main/template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimisation using Stan @ StanCon 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use a repository of pre-built package binaries to speed-up installation\n",
    "download.file(\"https://github.com/eddelbuettel/r2u/raw/master/inst/scripts/add_cranapt_jammy.sh\",\n",
    "              \"add_cranapt_jammy.sh\")\n",
    "Sys.chmod(\"add_cranapt_jammy.sh\", \"0755\")\n",
    "system(\"./add_cranapt_jammy.sh\")\n",
    "\n",
    "# Install R Packages\n",
    "install.packages(c(\"here\", \"tidyverse\", \"ggplot2\", \"MASS\", \"bayesplot\", \"cmdstanr\"),\n",
    "                  repos = c(\"https://stan-dev.r-universe.dev\", getOption(\"repos\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial, we use the [cmdstanr](https://mc-stan.org/cmdstanr/articles/cmdstanr.html) R interface to CmdStan. We install and setup CmdStan as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install and setup CmdStan\n",
    "download.file(\"https://github.com/stan-dev/cmdstan/releases/download/v2.35.0/colab-cmdstan-2.35.0.tgz\",\n",
    "              \"cmdstan-2.35.0.tgz\")\n",
    "utils::untar(\"cmdstan-2.35.0.tgz\")\n",
    "cmdstanr::set_cmdstan_path(\"cmdstan-2.35.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load all required libraries and set a seed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(here)\n",
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(MASS)\n",
    "library(cmdstanr)\n",
    "library(khroma)\n",
    "\n",
    "set.seed(424242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Icebreaker \n",
    "\n",
    "Imagine you only observe three function evaluations of an otherwise unknown function, and you want to find the global minimum of the function. \n",
    "\n",
    "How would you approach this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "observed_evals <- data.frame(x = c(0.2, 0.25, 0.8), y = c(0, -0.2, 5))\n",
    "\n",
    "ggplot(data = observed_evals, aes(x = x, y = y)) +\n",
    "    geom_point() + \n",
    "    ylab(\"f(x)\") +\n",
    "    ylim(-5, 5) +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Introduction to Bayesian optimisation (BO)\n",
    "\n",
    "Luckily, we don't have to continue guessing what points to evaluate next. Bayesian optimisation to the rescue! \n",
    "\n",
    "The goal of BO is to find the minimum or maximum of an unknown function for which we can only obtain function evaluations at a finite number of points. \n",
    "\n",
    "There are many applications of BO ranging from hyperparameter discovery to experimental design. \n",
    "\n",
    "The BO mechanism makes use of a **surrogate model** and an **acquisition function** to efficiently navigate the trade-off between exploration and exploitation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Surrogate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option for a surrogate model is a Gaussian process.  \n",
    "\n",
    "By definition of a GP, writing $g \\sim \\mathcal{GP}\\left(\\mu, K\\right)$ with mean function $\\mu(.)$ and a covariance or kernel function $K(.,.)$ means that the joint distribution of the function’s value $g(\\mathbf{x})$ at a finite number of input points $\\mathbf{x} = \\{x_1, \\cdots, x_n\\}$ is a multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Hands-on Exercise: implement your own kernel in Stan  \n",
    "# TODO: add an example for inference from GP kernel function & visualisation\n",
    "\n",
    "# squared exponential kernel \n",
    "\n",
    "# rational quadratic kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how to setup the different components of Bayesian optimisation, assume that the unknown function is $f(x) = (6  x - 2)^2  \\sin(12  x - 4)$ (aka Forrester function). We can start with the following GP surrogate for $f(x)$:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y &\\sim \\text{N}(g(x), \\sigma) \\ \\text{with} \\ \\sigma \\sim \\text{N}^+(0,1),\\\\\n",
    "\\\\\n",
    "g(x) &\\sim GP(\\mu, K),  \\text{with} \\ \\mu \\sim \\text{N}(0,1),\\\\ \n",
    "K_{i,j} &= k (x_i, x_j) = \\alpha^2  \\exp \\left(- \\frac{(x_i - x_j)^2}{\\rho^2} \\right),\\\\\n",
    "\\\\\n",
    "\\alpha &\\sim \\text{N}^+(0,1),\\\\ \n",
    "\\rho &\\sim \\text{N}(0.3,0.1).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stan, we can implement the model like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "stan_program <- \"\n",
    "data {\n",
    "  int<lower=1> N_obs;\n",
    "  array[N_obs] real x_obs;\n",
    "  vector[N_obs] y_obs;\n",
    "  int<lower=1> N_pred;\n",
    "  array[N_pred] real x_pred;\n",
    "}\n",
    "\n",
    "transformed data {\n",
    "  int<lower=1> N = N_obs + N_pred;\n",
    "  array[N] real x;\n",
    "  for (n_obs in 1:N_obs)   x[n_obs] = x_obs[n_obs];\n",
    "  for (n_pred in 1:N_pred) x[N_obs + n_pred] = x_pred[n_pred];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> rho;\n",
    "  real<lower=0> alpha;\n",
    "  real<lower=0> sigma;\n",
    "  real mu;\n",
    "  vector[N] eta;\n",
    "}\n",
    "\n",
    "transformed parameters{\n",
    "  vector[N] g;\n",
    "  { \n",
    "    matrix[N, N] L;\n",
    "    matrix[N, N] K;\n",
    "    K = gp_exp_quad_cov(x, alpha, rho) + diag_matrix(rep_vector(1e-10, N));\n",
    "    L = cholesky_decompose(K);\n",
    "    g = mu + L * eta;\n",
    "  }\n",
    "}\n",
    "\n",
    "model {\n",
    "  rho   ~ normal(0.3,0.1);\n",
    "  alpha ~ std_normal();\n",
    "  sigma ~ std_normal();\n",
    "  mu    ~ std_normal();\n",
    "  eta   ~ std_normal();\n",
    "  y_obs ~ normal(g[1:N_obs], sigma);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  vector[N_pred] y_pred;\n",
    "  for (n_pred in 1:N_pred){\n",
    "    y_pred[n_pred] = normal_rng(g[N_obs + n_pred], sigma);\n",
    "  }\n",
    "}\n",
    "\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this one implementation to get started, and there are other (more efficient) ways to set up GPs in Stan, see the comments by Andrew Johnson and Aki Vehtari in [Stan discourse]() and Aki's case study on GPs [here](). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Save Stan code in Stan file \n",
    "fit_gp <- cmdstanr::write_stan_file(stan_program, dir = \"Stan/\", basename = \"fit_gp.stan\")\n",
    "\n",
    "# To print Stan code, run: cat(readLines(fit_gp, sep = \"\\n\"))\n",
    "\n",
    "# To use the model, we first need to compile it\n",
    "model_fit_gp <- cmdstanr::cmdstan_model(fit_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Acquisition functions\n",
    "-> if I create a figure, put the code in the template such that participants can try\n",
    "\n",
    "#### For example: Lower confidence bound \n",
    "\n",
    "$$\\text{AF}_\\text{LCB}(x) = \\mu(x) - \\kappa*\\sigma(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lower_bound_acqs_fun <- function(m, s, kappa=2){\n",
    "  lower_bound <- m - kappa * s\n",
    "  return(lower_bound)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (?) Computational Tricks for GPs \n",
    "\n",
    "- mention GP tricks: Kronecker, HSGP\n",
    "- Thompson sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Cost- and response propensity-aware BO \n",
    "\n",
    "#### Varying cost of queries  \n",
    "\n",
    "#### Propensity of response \n",
    "\n",
    "-> possibly provide samples via GitHub & then ask to visualise \n",
    "-> Aalto file sharing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "true_f <- function(x) \n",
    "x_grid <- seq(0, 1, length.out = 100)\n",
    "f_evals <- true_f(x_grid)\n",
    "data_plot <- data.frame(x_grid, f_evals)\n",
    "\n",
    "# Plot the objective function \n",
    "plot <- ggplot(data = data_plot, aes(x = x_grid, y = f_evals)) +\n",
    "  geom_line() +\n",
    "  labs(y = \"f(x)\") +\n",
    "  theme_bw()\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we have collected any data, we can sample from our priors and check the predictions we would obtain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get samples using chosen priors \n",
    "\n",
    "model_sim <- cmdstanr::cmdstan_model(stan_file = here::here(\"Stan\", \"sim_gauss.stan\"))\n",
    "\n",
    "n_draws <- 15\n",
    "samples <- matrix(NA, nrow = n_draws, ncol=length(x_grid))\n",
    "\n",
    "for (i in 1:n_draws){\n",
    "    # 1. create data input, sample from the chosen priors for fit_gauss_3.stan\n",
    "    stan_dat <- list(N = length(x_grid),\n",
    "                     x = x_grid,\n",
    "                     alpha = abs(rnorm(1)),\n",
    "                     rho = rnorm(1, 0.3, 0.1),\n",
    "                     mu = rnorm(1),\n",
    "                     sigma = abs(rnorm(1, 0.1, 1)))\n",
    "\n",
    "    # 2. sample from model_sim using one chain and iteration, no warmup\n",
    "    gp_priors <- model_sim$sample(data = stan_dat,\n",
    "                                  seed = 424242, \n",
    "                                  iter_sampling = 1, \n",
    "                                  iter_warmup = 0, \n",
    "                                  chains = 1,\n",
    "                                  adapt_engaged=FALSE) # to sample without warmup\n",
    "    # 3. extract corresponding samples\n",
    "    samples[i,] <- gp_priors$draws(\"g\") \n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Visualise the prior predictive results \n",
    "\n",
    "\n",
    "# Extract and reformat samples\n",
    "data <- data.frame(samples[,1:NCOL(samples)])\n",
    "colnames(data) <- sub(\"^X\", \"\", colnames(data))\n",
    "\n",
    "data <- data |>\n",
    "  mutate(draw_id = as.factor(row_number())) |>\n",
    "  pivot_longer(cols = -draw_id, names_to = \"n_evals\", values_to = \"evaluations\") |>\n",
    "  mutate(n_evals = as.integer(n_evals)) |>\n",
    "  nest(data = c(draw_id, evaluations)) |>\n",
    "  mutate(x_grid = x_grid) |>\n",
    "  unnest(cols = c(data))\n",
    "\n",
    "# Plot prior predictives \n",
    "plot_prior_pred <- plot +\n",
    "  geom_line(data = data, aes(x = x_grid, y = evaluations, group = draw_id, color = draw_id, alpha = 0.4)) + \n",
    "  scale_colour_discreterainbow() + \n",
    "  theme(legend.position = \"none\")\n",
    "\n",
    "plot_prior_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Wrap-up \n",
    "\n",
    "- query from us function evaluations\n",
    "- find the minimum of a function, but this time use the science you’ve learnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

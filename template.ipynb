{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/annariha/StanCon-2024-BO-Stan/blob/main/template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimisation using Stan @ StanCon 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use a repository of pre-built package binaries to speed-up installation\n",
    "download.file(\"https://github.com/eddelbuettel/r2u/raw/master/inst/scripts/add_cranapt_jammy.sh\",\n",
    "              \"add_cranapt_jammy.sh\")\n",
    "Sys.chmod(\"add_cranapt_jammy.sh\", \"0755\")\n",
    "system(\"./add_cranapt_jammy.sh\")\n",
    "\n",
    "# Install R Packages\n",
    "install.packages(c(\"here\", \"tidyverse\", \"ggplot2\", \"MASS\", \"bayesplot\", \"cmdstanr\"),\n",
    "                  repos = c(\"https://stan-dev.r-universe.dev\", getOption(\"repos\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial, we use the [cmdstanr](https://mc-stan.org/cmdstanr/articles/cmdstanr.html) R interface to CmdStan. We install and setup CmdStan as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install and setup CmdStan\n",
    "download.file(\"https://github.com/stan-dev/cmdstan/releases/download/v2.35.0/colab-cmdstan-2.35.0.tgz\",\n",
    "              \"cmdstan-2.35.0.tgz\")\n",
    "utils::untar(\"cmdstan-2.35.0.tgz\")\n",
    "cmdstanr::set_cmdstan_path(\"cmdstan-2.35.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load all required libraries and set a seed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(here)\n",
    "library(tidyverse)\n",
    "library(ggplot2)\n",
    "library(MASS)\n",
    "library(cmdstanr)\n",
    "library(khroma)\n",
    "\n",
    "set.seed(424242)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Icebreaker \n",
    "\n",
    "Imagine you only observe three function evaluations of an otherwise unknown function, and you want to find the global minimum of the function. \n",
    "\n",
    "How would you approach this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "observed_evals <- data.frame(x = c(0.2, 0.25, 0.8), y = c(0, -0.2, 5))\n",
    "\n",
    "ggplot(data = observed_evals, aes(x = x, y = y)) +\n",
    "    geom_point() + \n",
    "    ylab(\"f(x)\") +\n",
    "    ylim(-5, 5) +\n",
    "    theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Introduction to Bayesian optimisation (BO)\n",
    "\n",
    "Luckily, we don't have to continue guessing what points to evaluate next. Bayesian optimisation to the rescue! \n",
    "\n",
    "The goal of BO is to find the minimum or maximum of an unknown function (\"black-box\") for which we can only obtain function evaluations at a finite number of points. \n",
    "\n",
    "There are many applications of BO ranging from hyperparameter discovery to experimental design. \n",
    "\n",
    "The BO mechanism makes use of a **surrogate model** and an **acquisition function** to efficiently navigate the trade-off between exploration and exploitation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Surrogate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option for a surrogate model is a Gaussian process.  \n",
    "\n",
    "By definition of a GP, writing $g \\sim \\mathcal{GP}\\left(\\mu, K\\right)$ with mean function $\\mu(.)$ and a covariance or kernel function $K(.,.)$ means that the joint distribution of the function’s value $g(\\mathbf{x})$ at a finite number of input points $\\mathbf{x} = \\{x_1, \\cdots, x_n\\}$ is a multivariate normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different covariance functions for GPs are available in Stan and are listed in the [Stan function documentation](https://mc-stan.org/docs/functions-reference/matrix_operations.html#gaussian-process-covariance-functions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how to set up the different components of Bayesian optimisation using a GP with a squared exponential covariance function, assume that the unknown function is $f(x) = (6  x - 2)^2  \\sin(12  x - 4)$, the so-called Forrester function. We can start with the following GP surrogate for $f(x)$:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y &\\sim \\text{N}(g(x), \\sigma) \\ \\text{with} \\ \\sigma \\sim \\text{N}^+(0,1),\\\\\n",
    "\\\\\n",
    "g(x) &\\sim GP(\\mu, K),  \\text{with} \\ \\mu \\sim \\text{N}(0,1),\\\\ \n",
    "K_{i,j} &= k (x_i, x_j) = \\alpha^2  \\exp \\left(- \\frac{(x_i - x_j)^2}{\\rho^2} \\right),\\\\\n",
    "\\\\\n",
    "\\alpha &\\sim \\text{N}^+(0,1),\\\\ \n",
    "\\rho &\\sim \\text{N}(0.3,0.1).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stan, we can implement the model like this, using the covariance function `gp_exp_quad_cov`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "stan_1_gp <- \"\n",
    "data {\n",
    "  int<lower=1> N_obs;\n",
    "  array[N_obs] real x_obs;\n",
    "  vector[N_obs] y_obs;\n",
    "  int<lower=1> N_pred;\n",
    "  array[N_pred] real x_pred;\n",
    "}\n",
    "\n",
    "transformed data {\n",
    "  int<lower=1> N = N_obs + N_pred;\n",
    "  array[N] real x;\n",
    "  for (n_obs in 1:N_obs)   x[n_obs] = x_obs[n_obs];\n",
    "  for (n_pred in 1:N_pred) x[N_obs + n_pred] = x_pred[n_pred];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> rho;\n",
    "  real<lower=0> alpha;\n",
    "  real<lower=0> sigma;\n",
    "  real mu;\n",
    "  vector[N] eta;\n",
    "}\n",
    "\n",
    "transformed parameters{\n",
    "  vector[N] g;\n",
    "  { \n",
    "    matrix[N, N] L;\n",
    "    matrix[N, N] K;\n",
    "    K = gp_exp_quad_cov(x, alpha, rho) + diag_matrix(rep_vector(1e-10, N));\n",
    "    L = cholesky_decompose(K);\n",
    "    g = mu + L * eta;\n",
    "  }\n",
    "}\n",
    "\n",
    "model {\n",
    "  rho   ~ normal(0.3,0.1);\n",
    "  alpha ~ std_normal();\n",
    "  sigma ~ std_normal();\n",
    "  mu    ~ std_normal();\n",
    "  eta   ~ std_normal();\n",
    "  y_obs ~ normal(g[1:N_obs], sigma);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  vector[N_pred] y_pred;\n",
    "  for (n_pred in 1:N_pred){\n",
    "    y_pred[n_pred] = normal_rng(g[N_obs + n_pred], sigma);\n",
    "  }\n",
    "}\n",
    "\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this one implementation to get started, and there are other (more efficient) ways to set up and reparameterise GPs in Stan, see the comments by Andrew Johnson and Aki Vehtari in [Stan discourse](https://discourse.mc-stan.org/t/help-reparameterize-gp-model-to-remove-divergent-transitions/26425/4?u=andrjohns), Aki's case study on GPs [here](https://avehtari.github.io/casestudies/Motorcycle/motorcycle_gpcourse.html), and the computational tricks mentioned in the tutorial slides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Save Stan code in Stan file \n",
    "fit_1_gp <- cmdstanr::write_stan_file(stan_1_gp, dir = \"Stan/\", basename = \"fit_1_gp.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# To use the model, we first need to compile it\n",
    "model_fit_1_gp <- cmdstanr::cmdstan_model(fit_1_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use the model\n",
    "\n",
    "# 1. Data input according to our assumed priors above\n",
    "stan_1_dat <- list(N = length(x_grid),\n",
    "                   x = x_grid,\n",
    "                   alpha = abs(rnorm(1)),\n",
    "                   rho = rnorm(1, 0.3, 0.1),\n",
    "                   mu = rnorm(1),\n",
    "                   sigma = abs(rnorm(1, 0.1, 1)))\n",
    "\n",
    "# 2. Get samples from model \n",
    "gp_samples <- model_fit_1_gp$sample(data = stan_dat,\n",
    "                                    seed = 424242,\n",
    "                                    iter_sampling = 1000,\n",
    "                                    chains = 4)\n",
    "\n",
    "# 3. Extract draws \n",
    "samples <- gp_priors$draws(\"g\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn: \n",
    "\n",
    "1. Adjust the above Stan code such that it uses the Matérn 3/2 covariance function instead;\n",
    "2. If you have been working in a code cell here, make sure to save the Stan code in a Stan file; \n",
    "3. Compile & use the model. \n",
    "\n",
    "The Matérn 3/2 covariance function is given by: \n",
    "\n",
    "$$k(\\mathbf{x}_i, \\mathbf{x}_j) = \\sigma^2 \\left( 1 + \\frac{\\sqrt{3}|\\mathbf{x}_i - \\mathbf{x}_j|}{l} \\right) \\exp \\left( -\\frac{\\sqrt{3}|\\mathbf{x}_i - \\mathbf{x}_j|}{l} \\right)$$\n",
    "\n",
    "Our model with Matérn covariance function, requires us to choose priors for the parameters $\\sigma$ and $l$. Let's assume the following priors for now: \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\sigma &\\sim \\text{N}^+(0,1)\\\\\n",
    "l &\\sim ...\n",
    "\\end{aligned} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Your Stan code goes here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Save Stan code in Stan file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Use the model\n",
    "\n",
    "# 1. Data input according to your assumed priors\n",
    "\n",
    "# 2. Get samples from model\n",
    "\n",
    "# 3. Extract draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Acquisition functions\n",
    "\n",
    "Acquisition functions serve as a guide to efficiently decide where to query the unknown function next. Different considerations about the problem at hand can motivate choosing different acquisition functions. \n",
    "\n",
    "#### For example: Lower confidence bound \n",
    "\n",
    "$$\\text{AF}_\\text{LCB}(x) = \\mu(x) - \\kappa*\\sigma(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lower_bound_acqs_fun <- function(m, s, kappa=2){\n",
    "  lower_bound <- m - kappa * s\n",
    "  return(lower_bound)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cost- and response propensity-aware BO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying cost of queries\n",
    "\n",
    "Instead of assuming that all queries have the same cost, we can build our approach such that we allow varying cost of queries. \n",
    "\n",
    "We can include this in the BO loop as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propensity of response \n",
    "\n",
    "In some applications, it is important to account for the fact that we might not obtain a response at the point where we decided to query next. For example, if we send out a survey, there will be people that won't respond, and it might be that one group of people is overall less likely to respond than another, for example, based on age or gender.  \n",
    "\n",
    "We can include this in the BO loop as follows: \n",
    "\n",
    "-> possibly provide samples via GitHub & then ask to visualise \n",
    "-> Aalto file sharing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "true_f <- function(x) \n",
    "x_grid <- seq(0, 1, length.out = 100)\n",
    "f_evals <- true_f(x_grid)\n",
    "data_plot <- data.frame(x_grid, f_evals)\n",
    "\n",
    "# Plot the objective function \n",
    "plot <- ggplot(data = data_plot, aes(x = x_grid, y = f_evals)) +\n",
    "  geom_line() +\n",
    "  labs(y = \"f(x)\") +\n",
    "  theme_bw()\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we have collected any data, we can sample from our priors and check the predictions we would obtain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Get samples using chosen priors \n",
    "\n",
    "model_sim <- cmdstanr::cmdstan_model(stan_file = here::here(\"Stan\", \"sim_gauss.stan\"))\n",
    "\n",
    "n_draws <- 15\n",
    "samples <- matrix(NA, nrow = n_draws, ncol=length(x_grid))\n",
    "\n",
    "for (i in 1:n_draws){\n",
    "    # 1. create data input, sample from the chosen priors for fit_gauss_3.stan\n",
    "    stan_dat <- list(N = length(x_grid),\n",
    "                     x = x_grid,\n",
    "                     alpha = abs(rnorm(1)),\n",
    "                     rho = rnorm(1, 0.3, 0.1),\n",
    "                     mu = rnorm(1),\n",
    "                     sigma = abs(rnorm(1, 0.1, 1)))\n",
    "\n",
    "    # 2. sample from model_sim using one chain and iteration, no warmup\n",
    "    gp_priors <- model_sim$sample(data = stan_dat,\n",
    "                                  seed = 424242, \n",
    "                                  iter_sampling = 1, \n",
    "                                  iter_warmup = 0, \n",
    "                                  chains = 1,\n",
    "                                  adapt_engaged=FALSE) # to sample without warmup\n",
    "    # 3. extract corresponding samples\n",
    "    samples[i,] <- gp_priors$draws(\"g\") \n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Visualise the prior predictive results \n",
    "\n",
    "\n",
    "# Extract and reformat samples\n",
    "data <- data.frame(samples[,1:NCOL(samples)])\n",
    "colnames(data) <- sub(\"^X\", \"\", colnames(data))\n",
    "\n",
    "data <- data |>\n",
    "  mutate(draw_id = as.factor(row_number())) |>\n",
    "  pivot_longer(cols = -draw_id, names_to = \"n_evals\", values_to = \"evaluations\") |>\n",
    "  mutate(n_evals = as.integer(n_evals)) |>\n",
    "  nest(data = c(draw_id, evaluations)) |>\n",
    "  mutate(x_grid = x_grid) |>\n",
    "  unnest(cols = c(data))\n",
    "\n",
    "# Plot prior predictives \n",
    "plot_prior_pred <- plot +\n",
    "  geom_line(data = data, aes(x = x_grid, y = evaluations, group = draw_id, color = draw_id, alpha = 0.4)) + \n",
    "  scale_colour_discreterainbow() + \n",
    "  theme(legend.position = \"none\")\n",
    "\n",
    "plot_prior_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Wrap-up \n",
    "\n",
    "- query from us function evaluations\n",
    "- find the minimum of a function, but this time use the science you’ve learnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

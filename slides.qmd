---
title: "Bayesian optimisation using Stan"
author: 
  - name: Anna Elisabeth Riha
    affiliations:
      - name: Aalto University
  - name: Adam Howes
    affiliations:
      - name: Independent
  # - name: Aki Vehtari
  #   affiliations:
  #     - name: Aalto University
  - name: Seth Flaxman
    affiliations:
      - name: University of Oxford
  - name: Elizaveta Semenova
    affiliations:
      - name: Imperial College London
date: 09/13/2024
date-format: long
format: 
  beamer:
    navigation: horizontal
    aspectratio: 169
    theme: metropolis
    header-includes: |
        \newcommand{\theHtable}{\thetable} % fix for this issue https://github.com/quarto-dev/quarto-cli/issues/10019
        \setbeamercolor{frametitle}{bg=white,fg=black}
        \setbeamercolor{sectiontitle}{bg=white,fg=black}
        \usepackage{cmbright} % european computer modern bright font
        \usepackage[T1]{fontenc} % european computer modern bright font
        \usepackage[utf8]{inputenc}
        \usepackage[labelformat=empty]{caption}
        \usepackage[export]{adjustbox}
        \titlegraphic{
          \vspace{5.5cm}
          \begin{figure}
            \centering 
            \includegraphics[width=0.25\paperwidth, valign=c]{images/Imperial_logo.png}
            \includegraphics[width=0.27\paperwidth, valign=c]{images/aaltologo.pdf}
            \includegraphics[width=0.25\paperwidth, valign=c]{images/FCAI_logo_purple.png}
          \end{figure}
        }
    bibliography: references.bib
    biblio-style: plain
    urlcolor: blue
    mathspec: true
---


## Bayesian optimisation using Stan

:::: {.columns}

::: {.column width="48%"}

<!--![](images/bo_demo.png){width=100%}-->
(a nice BO image here)

:::

::: {.column width="50%"}

In this tutorial, we will  

- provide an overview of Bayesian optimisation (BO),
- demonstrate how Stan can be used within the BO procedure, 
- demonstrate variable query cost and non-response propensity within BO.

:::

::::



## Learning outcomes for today's tutorial

After this session, you will be able to 

- formulate the main goal and components of BO,
- implement Gaussian process surrogates,
- use several acquisution functions within BO.

## Schedule for today's tutorial

| Time   | Activity                              |
|--------|---------------------------------------|
| 5 min  | Warmup                                |
| 20 min | Introduction to Bayesian optimisation |
| 20 min | Surrogates                            |
| 10 min | Acquisition functions                 |
| 10 min | Break                                 |
| 5 min  | Computational tricks                  |
| 10 min | Cost- and propensity-aware BO         |
| 20 min | Hands-on practice                      |

# Warmup

## Where to sample next to find global minimum? 
<!-- Ice-breaker: we show you function evaluation, tell us where to sample next to find global minimum 
-->
TODO

# What does Bayesian optimisation solve? 

## Problem definition: global optimisation

The goal of **global optimisation** of a real-valued function $f: \mathcal{X} \to \mathbb{R}$ is to find a *minimiser* $x^*$ (there may be more than one) in the search space $\mathcal{X}$, such that:

$$
x^* = \text{arg min}_{x \in \mathcal{X}} f(x).
$$

## Problem definition: global optimisation

Today we focus on finding a minimum, but finding a maximum can be approached in the same way since 
$$\text{max}_{x \in \mathcal{X}} f(x) = - \text{min}_{x \in \mathcal{X}}f(x).$$

## Problem definition: global optimisation

The function $f$ to be optimised is referred to as the **objective function**. 

In contrast to *local optimisation*, global optimisation requires that 
$$f(x^*) \leq f(x)$$ 
for **all** $x \in \mathcal{X}$ rather than only in some neighbourhood about $x^*$. Throughout this workshop, we assume that the search space $\mathcal{X}$ is a subset of $\mathbb{R}^d$ where $d \in \mathbb{N}$:
$$\mathcal{X} \subset  \mathbb{R}^d$$

## Group discussion

Given a function $f: \mathcal{X} \to \mathbb{R},$ how would you approach the search of its minimum?

## Problem definition: global optimisation

In practice, the objective function $f$ may possess the following challenging properties:

1. *Non-linear, non-convex*,

## Problem definition: global optimisation

In practice, the objective function $f$ may possess the following challenging properties:

2. *Black-box:* A function is called **black-box** if it can only be viewed in terms of its inputs and outputs. If $f$ is black-box then it does not have an analytic form or derivatives, such as the gradient $\nabla f$ or Hessian $\mathbf{H}$.

## Problem definition: global optimisation

> "Any sufficiently complex system acts as a black-box when it becomes easier to experiment with than to understand."
 
::: {.flushright data-latex=""}
Golovin et al, "Google Vizier" (2017)
:::

## Problem definition: global optimisation

In practice, the objective function $f$ may possess the following challenging properties:

3. *Expensive to evaluate*: The sampling procedure is computationally, economically or otherwise prohibitively expensive. 

## Problem definition: global optimisation

In practice, the objective function $f$ may possess the following challenging properties:

4. *Noisy*: When $f$ is evaluated at $x$, the value returned $y$ is contaminated by noise $\epsilon$, typically assumed to be Gaussian with zero mean and variance $\sigma^2$ such that 
$$y = f(x) + \epsilon$$.

## Problem definition: global optimisation

- Perhaps, the global optimisation problem can be solved using sampling!

## Sample designs

- How should points be queried to efficiently learn about $x^*$?

## Sample designs

- How should points be queried to efficiently learn about $x^*$?

- Let's focus on finding a ``good" solution or converging to a minimiser $x^*$ in few evaluations, rather than in making theoretical guarantees about optimality.

## Sample designs

The two relatively naive strategies:

- *grid-search*, 

- *random-search*.

## Sample designs: grid search

*Grid-search*:

- **How**: Samples are taken spaced evenly throughout the domain $\mathcal{X}$ at a resolution appropriate to the optimisation budget.

- **Pitfalls**: Although the whole domain is superficially covered, if few function evaluations can be afforded then this coverage is too sparse to reliably locate a minimiser.

## Sample designs: random search

*Random-search*:

- **How**: random-search chooses inputs in the domain $\mathcal{X}$ to evaluate at random.

- **Pitfalls**: complete randomness lends itself to clumps of points and large areas left empty.

## Sample designs: latin-hypercube

*Latin-hypercube*:

a grid with exactly one sample in each column and each row. This avoids the problem of collapsing, from which grid-search suffers. 

## Sample designs: static designs

An issue with the aforementioned strategies: information gained during the search is not used to better inform the next decision.

## Sample designs: sequential decision making

Rather than choose all the points at once it makes sense instead to consider a *sequential decision making* problem where at each stage a point is carefully chosen to be evaluated next. 

## Sample designs: sequential decision making

So, how can previous information be used?

## Sequential decision making: idea

::: {.incremental}
- Make assumptions about $f$, e.g. that $f$ is smooth.
- Build a model which mimics behaviour of $f$. A model with uncertainty!
- Sample in uncertain regions, not near to any previous evaluations, to *explore*.
- Sample in promising regions, near to previous low evaluations, to *exploit*. 
::: 


## Sequential decision making: idea

This is exactly how Bayesian optimisation works.

# The building blocks of Bayesian optimisation

## Bayesian optimisation use cases

## Bayesian optimisation use cases

## Connection to active learning

## The BO loop 

## Let's take a break! (10 min)

Some suggestions for recharging during breaks :)  

- move your body
- open a window or go outside 
- drink some water 
- try to avoid checking e-mails, messengers, or social media

# Gaussian Processes as surrogates

# A primer on XYZ

## XYZ 

$$
p(\theta \mid y) \propto p(\theta) p(y \mid \theta)
$$

## XYZ (ctd.)

```{r}
head(mtcars)
```

## Summary

What we learnt today:

::: {.incremental}
- BO 
- BO 
- BO 
::: 

# More resources

## Reading materials

- a paper on ...: 
- a paper on ...: 

## Case studies

- a case study on ...: [add url]()
- a case study on ...: [add url]()

## References


## Stay tuned

We will soon release a write-up based on the materials of this workshop, with more details.


@misc{forrester_engineering_2008,
	title = {Engineering {Design} via {Surrogate} {Modelling}: {A} {Practical} {Guide} {\textbar} {Wiley}},
	shorttitle = {Engineering {Design} via {Surrogate} {Modelling}},
	url = {https://www.wiley.com/en-gb/Engineering+Design+via+Surrogate+Modelling%3A+A+Practical+Guide-p-9780470060681},
	abstract = {Surrogate models expedite the search for promising designs by standing in for expensive design evaluations or simulations. They provide a global model of some metric of a design (such as weight, aerodynamic drag, cost, etc.), which can then be optimized efficiently. Engineering Design via Surrogate Modelling is a self-contained guide to surrogate models and their use in engineering design. The fundamentals of building, selecting, validating, searching and refining a surrogate are presented in a manner accessible to novices in the field. Figures are used liberally to explain the key concepts and clearly show the differences between the various techniques, as well as to emphasize the intuitive nature of the conceptual and mathematical reasoning behind them. More advanced and recent concepts are each presented in stand-alone chapters, allowing the reader to concentrate on material pertinent to their current design problem, and concepts are clearly demonstrated using simple design problems. This collection of advanced concepts (visualization, constraint handling, coping with noisy data, gradient-enhanced modelling, multi-fidelity analysis and multiple objectives) represents an invaluable reference manual for engineers and researchers active in the area. Engineering Design via Surrogate Modelling is complemented by a suite of Matlab codes, allowing the reader to apply all the techniques presented to their own design problems. By applying statistical modelling to engineering design, this book bridges the wide gap between the engineering and statistics communities. It will appeal to postgraduates and researchers across the academic engineering design community as well as practising design engineers. Provides an inclusive and practical guide to using surrogates in engineering design. Presents the fundamentals of building, selecting, validating, searching and refining a surrogate model. Guides the reader through the practical implementation of a surrogate-based design process using a set of case studies from real engineering design challenges. Accompanied by a companion website featuring Matlab software at http://www.wiley.com/go/forrester},
	language = {en-gb},
	urldate = {2023-07-12},
	journal = {Wiley.com},
	author = {Forrester, Alexander and Sobester, András and Keane, Andy},
	month = sep,
	year = {2008},
}

@misc{betancourt_robust_nodate,
	title = {Robust {Gaussian} {Process} {Modeling}},
	url = {https://betanalpha.github.io/assets/case_studies/gaussian_processes.html},
	urldate = {2023-07-12},
	author = {Betancourt, Michael},
}

@book{rasmussen_gaussian_2005,
	title = {Gaussian {Processes} for {Machine} {Learning}},
	isbn = {978-0-262-25683-4},
	url = {https://direct.mit.edu/books/book/2320/Gaussian-Processes-for-Machine-Learning},
	abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machi},
	language = {en},
	urldate = {2023-07-12},
	publisher = {The MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	month = nov,
	year = {2005},
	doi = {10.7551/mitpress/3206.001.0001},
}

@article{austin_introduction_2011,
	title = {An {Introduction} to {Propensity} {Score} {Methods} for {Reducing} the {Effects} of {Confounding} in {Observational} {Studies}},
	volume = {46},
	issn = {0027-3171},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3144483/},
	doi = {10.1080/00273171.2011.568786},
	abstract = {The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed baseline covariates will be similar between treated and untreated subjects. I describe 4 different propensity score methods: matching on the propensity score, stratification on the propensity score, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. I describe balance diagnostics for examining whether the propensity score model has been adequately specified. Furthermore, I discuss differences between regression-based methods and propensity score-based methods for the analysis of observational data. I describe different causal average treatment effects and their relationship with propensity score analyses.},
	number = {3},
	urldate = {2023-07-12},
	journal = {Multivariate Behavioral Research},
	author = {Austin, Peter C.},
	month = may,
	year = {2011},
	pmid = {21818162},
	pmcid = {PMC3144483},
	pages = {399--424},
}

@book{williamson_bayesian_2022,
	address = {Cambridge},
	title = {Bayesian {Methods} for {Interaction} and {Design}},
	isbn = {978-1-108-83499-5},
	url = {https://www.cambridge.org/core/books/bayesian-methods-for-interaction-and-design/721123C200F67FD94DA8DDFD561162A8},
	abstract = {Intended for researchers and practitioners in interaction design, this book shows how Bayesian models can be brought to bear on problems of interface design and user modelling. It introduces and motivates Bayesian modelling and illustrates how powerful these ideas can be in thinking about human-computer interaction, especially in representing and manipulating uncertainty. Bayesian methods are increasingly practical as computational tools to implement them become more widely available, and offer a principled foundation to reason about interaction design. The book opens with a self-contained tutorial on Bayesian concepts and their practical implementation, tailored for the background and needs of interaction designers. The contributed chapters cover the use of Bayesian probabilistic modelling in a diverse set of applications, including improving pointing-based interfaces; efficient text entry using modern language models; advanced interface design using cutting-edge techniques in Bayesian optimisation; and Bayesian approaches to modelling the cognitive processes of users.},
	urldate = {2023-07-12},
	publisher = {Cambridge University Press},
	editor = {Williamson, John H. and Oulasvirta, Antti and Kristensson, Per Ola and Banovic, Nikola},
	year = {2022},
	doi = {10.1017/9781108874830},
}

@book{sarkka_bayesian_2013,
	address = {Cambridge},
	series = {Institute of {Mathematical} {Statistics} {Textbooks}},
	title = {Bayesian {Filtering} and {Smoothing}},
	isbn = {978-1-107-03065-7},
	url = {https://www.cambridge.org/core/books/bayesian-filtering-and-smoothing/C372FB31C5D9A100F8476C1B23721A67},
	abstract = {Filtering and smoothing methods are used to produce an accurate estimate of the state of a time-varying system based on multiple observational inputs (data). Interest in these methods has exploded in recent years, with numerous applications emerging in fields such as navigation, aerospace engineering, telecommunications and medicine. This compact, informal introduction for graduate students and advanced undergraduates presents the current state-of-the-art filtering and smoothing methods in a unified Bayesian framework. Readers learn what non-linear Kalman filters and particle filters are, how they are related, and their relative advantages and disadvantages. They also discover how state-of-the-art Bayesian parameter estimation methods can be combined with state-of-the-art filtering and smoothing algorithms. The book's practical and algorithmic approach assumes only modest mathematical prerequisites. Examples include Matlab computations, and the numerous end-of-chapter exercises include computational assignments. Matlab code is available for download at www.cambridge.org/sarkka, promoting hands-on work with the methods.},
	urldate = {2023-07-12},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo},
	year = {2013},
	doi = {10.1017/CBO9781139344203},
}

@misc{lee_cost-aware_2020,
	title = {Cost-aware {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/2003.10870},
	doi = {10.48550/arXiv.2003.10870},
	abstract = {Bayesian optimization (BO) is a class of global optimization algorithms, suitable for minimizing an expensive objective function in as few function evaluations as possible. While BO budgets are typically given in iterations, this implicitly measures convergence in terms of iteration count and assumes each evaluation has identical cost. In practice, evaluation costs may vary in different regions of the search space. For example, the cost of neural network training increases quadratically with layer size, which is a typical hyperparameter. Cost-aware BO measures convergence with alternative cost metrics such as time, energy, or money, for which vanilla BO methods are unsuited. We introduce Cost Apportioned BO (CArBO), which attempts to minimize an objective function in as little cost as possible. CArBO combines a cost-effective initial design with a cost-cooled optimization phase which depreciates a learned cost model as iterations proceed. On a set of 20 black-box function optimization problems we show that, given the same cost budget, CArBO finds significantly better hyperparameter configurations than competing methods.},
	urldate = {2023-07-12},
	publisher = {arXiv},
	author = {Lee, Eric Hans and Perrone, Valerio and Archambeau, Cedric and Seeger, Matthias},
	month = mar,
	year = {2020},
	note = {arXiv:2003.10870 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{garnett-2023, place={Cambridge}, title={Bayesian Optimization}, DOI={10.1017/9781108348973}, publisher={Cambridge University Press}, author={Garnett, Roman}, year={2023}}

@online{stan-2024,
  title = {Stan {{Modeling Language Users Guide}} and {{Reference Manual}} 2.34},
  author = {{Stan Development Team}},
  date = {2024},
  url = {https://mc-stan.org},
  urldate = {2024-05-14},
  langid = {english},
  organization = {stan-dev.github.io}
}

@inproceedings{vapnik-svm-1997,
  title = {The {{Support Vector}} Method},
  booktitle = {Artificial {{Neural Networks}} {{ICANN}}'97},
  author = {Vapnik, Vladimir N.},
  editor = {Gerstner, Wulfram and Germond, Alain and Hasler, Martin and Nicoud, Jean-Daniel},
  date = {1997},
  pages = {261--271},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/BFb0020166},
  abstract = {The Support Vector (SV) method is a new general method of function estimation which does not depend explicitly on the dimensionality of input space. It was applied for pattern recognition, regression estimation, and density estimation problems as well as for problems of solving linear operator equations. In this article we describe the general idea of the SV method and present theorems demonstrating that the generalization ability of the SV method is based on factors which classical statistics do not take into account. We also describe the SV method for density estimation in a set of functions defined by a mixture of an infinite number of Gaussians.},
  isbn = {978-3-540-69620-9},
  langid = {english},
  keywords = {Generalization Ability,Kernel Representation,Optimal Hyperplane,Pattern Recognition Problem,Support Vector}
}

@article{breiman-2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  date = {2001-10-01},
  journaltitle = {Machine Learning},
  shortjournal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  url = {https://doi.org/10.1023/A:1010933404324},
  urldate = {2024-05-14},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148â€“156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  langid = {english},
  keywords = {classification,ensemble,regression}
}

@online{li-bnn-optim-2024,
  title = {A {{Study}} of {{Bayesian Neural Network Surrogates}} for {{Bayesian Optimization}}},
  author = {Li, Yucen Lily and Rudner, Tim G. J. and Wilson, Andrew Gordon},
  date = {2024-05-08},
  eprint = {2305.20028},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2305.20028},
  url = {http://arxiv.org/abs/2305.20028},
  urldate = {2024-05-14},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{bergstra-algorithms-2011,
  title = {Algorithms for {{Hyper-Parameter Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bergstra, James and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
  date = {2011},
  volume = {24},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html},
  urldate = {2024-05-14}
}

@article{gortler-2019,
  title = {A {{Visual Exploration}} of {{Gaussian Processes}}},
  author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
  date = {2019-04-02},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {4},
  number = {4},
  pages = {e17},
  issn = {2476-0757},
  doi = {10.23915/distill.00017},
  url = {https://distill.pub/2019/visual-exploration-gaussian-processes},
  urldate = {2024-05-14},
  abstract = {How to turn a collection of small building blocks into a versatile tool for solving regression problems.},
  langid = {english}
}

@misc{wilkinson-2020,
  author        = {Richard Wilkinson},
  title         = {Introduction to Gaussian Processes. Lecture Notes from GP Summer School.},
  month         = {September},
  year          = {2020},
  url           = {http://gpss.cc/gpss20/slides/Wilkinson2020.pdf},
  publisher={School of Mathematical Sciences, University of Nottingham}
}

@article{agnihotri2020,
  author = {Agnihotri, Apoorv and Batra, Nipun},
  title = {Exploring Bayesian Optimization},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/bayesian-optimization},
  doi = {10.23915/distill.00026}
}

@article{mockus1989global,
  title={Global optimization and the Bayesian approach},
  author={Mockus, Jonas},
  journal={Bayesian Approach to Global Optimization: Theory and Applications},
  pages={1--3},
  year={1989},
  publisher={Springer}
}

@inproceedings{balandat2020botorch,
  title = {{BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}},
  author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
  booktitle = {Advances in Neural Information Processing Systems 33},
  year = 2020,
  url = {http://arxiv.org/abs/1910.06403}
}

@Misc{gpyopt2016,
author = {The GPyOpt authors},
title = {{GPyOpt}: A Bayesian Optimization framework in python},
howpublished = {\url{http://github.com/SheffieldML/GPyOpt}},
year = {2016}
}

@Article{mockus-1978,
  author       = {Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
  title        = {The Application of {B}ayesian Methods for Seeking the Extremum},
  volume       = {2},
  number       = {117-129},
  pages        = {2},
  year         = {1978},
  journal = {Towards Global Optimization},
  publisher    = {Amsterdam: Elsevier},
}

@article{pricopie2024adaptive,
  title={An adaptive approach to Bayesian Optimization with switching costs},
  author={Pricopie, Stefan and Allmendinger, Richard and Lopez-Ibanez, Manuel and Fare, Clyde and Benatan, Matt and Knowles, Joshua},
  journal={arXiv preprint arXiv:2405.08973},
  year={2024}
}

@article{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{shahriari2015taking,
  title={Taking the human out of the loop: A review of Bayesian optimization},
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and De Freitas, Nando},
  journal={Proceedings of the IEEE},
  volume={104},
  number={1},
  pages={148--175},
  year={2015},
  publisher={IEEE}
}

@article{frazier2018tutorial,
  title={A tutorial on Bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}

@article{settles2009active,
  title={Active learning literature survey},
  author={Settles, Burr},
  year={2009},
  publisher={University of Wisconsin-Madison Department of Computer Sciences}
}

@article{kushner1964new,
  title={A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise},
  author={Kushner, Harold J},
  year={1964}
}

@article{srinivas2009gaussian,
  title={Gaussian process optimization in the bandit setting: No regret and experimental design},
  author={Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M and Seeger, Matthias},
  journal={arXiv preprint arXiv:0912.3995},
  year={2009}
}
